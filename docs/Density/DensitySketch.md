---
layout: doc_page
---
<!--
    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.
-->
## Contents
<!-- TOC -->
* [Density Sketch](#density-sketch)
    * [Foundation Paper](#paper)
        * [Key Highlights](#highlights)
        * [Practical Implications](#implications)
    * [Inspirational Code, Example and Tests](#inspiration)
<!-- TOC -->

<a id="density-sketch"></a>
## DensitySketch
**Quick summary:** This sketch builds a coreset from the given set of input points as multi-dimensional vectors. Provides density estimate at a given point.

<a id="paper"></a>
### Our implementation is based on the following paper:<br>

* Zohar Karnin, Edo Liberty "Discrepancy, Coresets, and Sketches in Machine Learning"
https://proceedings.mlr.press/v99/karnin19a/karnin19a.pdf
    * The paper explores the relationship between class discrepancy and the efficiency of data reduction techniques like coresets and streaming sketches.
    * The core contribution is a general framework for creating small, representative subsets of data (coresets) that maintain the statistical properties of the original large dataset.

<a id="highlights"></a>
#### Key Highlights:
* **New Complexity Measure:** The authors define "class discrepancy" as a way to characterize the coreset complexity of different function families, similar to how Rademacher complexity is used for generalization.
* **Improved Coreset Sizes:** They prove the existence of $\epsilon$-approximation coresets of size $O(\sqrt{d}/\epsilon)$ for several common machine learning problems, including:
    * Logistic regression
    * Sigmoid activation loss
    * Matrix covariance
    * Kernel density estimation
* **Gaussian Kernel Resolution:** The paper resolves a long-standing open problem by matching the lower bound for the coreset complexity of Gaussian kernel density estimation at $O(\sqrt{d}/\epsilon)$.
* **Streaming Algorithms:** It introduces an exponential improvement to the "merge-and-reduce" trick, leading to better streaming sketches for any problem with low discrepancy.
* **Deterministic Algorithm:** The authors provide a simple, deterministic algorithm for finding low-discrepancy sequences and coresets for any positive semi-definite kernel.

<a id="implications"></a>
#### Practical Implications:
The findings allow for significantly faster optimization in large-scale machine learning. By reducing a massive dataset into a much smaller coreset, researchers can perform complex calculations (like training a logistic regression model) with a fraction of the computational cost while maintaining a high level of accuracy.

<a id="inspiration"></a>
### Our implementations was inspired by the following implementation, example, and tests by Edo Liberty:
* **Code:** https://github.com/edoliberty/streaming-quantiles/blob/f688c8161a25582457b0a09deb4630a81406293b/gde.py
* **Example** https://github.com/edoliberty/streaming-quantiles/blob/f688c8161a25582457b0a09deb4630a81406293b/gde_example_usage.ipynb
* **Tests** https://github.com/edoliberty/streaming-quantiles/blob/f688c8161a25582457b0a09deb4630a81406293b/gde_test.py


*(Note: much of this overview was generated by Gemini AI, it may contain errors.)*
